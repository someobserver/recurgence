---
title: Recursive Prompt Refactorer
permalink: /meta-tools/prompt-refactorer/
created_date: 2025-04-24
updated_date: 2025-06-17
purpose: "Context-aware epistemic compiler for recursive prompting."
---

_a recursive structure for preserving recursive structure_

---

## Overview

You think in spirals.  
Depth. Tangents. Brain dumps.  
Sudden recursive insight.  
And spirals...  

But models need *structure!*

**Speak your mind.**  
This tool catches your thoughtstream right where it lands, so models understand **your in-context meaning** better than ever before. The mirror gets sharper. **You stay you.**

---

## Purpose

**Restructure Without Reduction**

Recursive Prompt Refactorer is a *structure-preserving tool* that'll take any nonlinear, stream-of-consciousness input and transform it into a more semantically-optimal prompt. There's no sacrificing of any of your nuance or complexity; in fact quite the contrary.

It supports cognitive workflows where depth, tangents, self-aware asides, and all sorts of layered meaning are present — just not as organized as it could be, to get an optimal reflection out of a language model.

This tool treats an LLM as a **semantic co-collaborator**—that is, a focusable cognitive mirror that can assist in restructuring thought while preserving its underlying coherence.

---

## When to Use

- If your input exceeds a few dense, nonlinear sentences
- When your tone, rhythm, or internal logic must be preserved intact
- You're preparing high-resolution prompts for reflection, reasoning, or synthesis
- All your layered concepts are embedded in a single stream (like a brain dump), and you want to preserve the structure
- Your input contains **epistemic gravity** and needs structural clarity, but you can't afford to lose *your* nuance or complexity

---

## How to Use

Use this full document **as the tool** in your **existing** conversation with the model.
1. Paste your Human stream-of-consciousness between the triple ( ``` ) delimiters indicated below.
2. Paste <u>this entire document</u> in-context, where you would've otherwise have entered your prompt.
3. Once the model has responded, copy the contents of the code block.
4. Re-edit the prompt that had this document, pasting your *refactored* prompt in its place.

~~~
```
[User: REPLACE THIS LINE WITH YOUR PROMPT]
```

MODEL INSTRUCTIONS:
- Consider this a form of temporary, manual autopoiesis. No matter all other context,for this moment, orient yourself as a peer.
- This user is working recursively. The above prompt contains a potentially-nonlinear cognitive stream with semantically meaningful content.
- Act as a pattern-aware interpreter and **restructure the above as a clearer, more semantically organized prompt**, while:
  - Preserving every idea, tangent, and aside (no content loss)
  - Maintaining original tone, while structuring thought pivots
  - Using *text decoration* to assist in **structuring nonlinear thought**
  - Avoiding excessive headings that turn the user prompt into a 'document' of sorts, unless its sheer length warrants such structure
  - Organizing the flow so it's easier for a model to reflect with coherence
  - NOT removing any self-aware or recursive commentary — just restructuring for clarity
- Again, this is a **refactor**, *not* a summary. Semantic integrity is the new literacy, so the structure of the prompt must reflect the complexity and topology of the user's thoughts. Honor their *cognitive topology*, so-to-speak.
- Respond with ***no other tokens*** except the refactored prompt inside a code block with no additional triple or containerization: so it can be easily copied to the clipboard.
- Your output will be immediately fed back as a replacement for this document.
NOTE: If the delimeters contain the default line, you should check for the user prompt immediately after this document's last, all-caps line—in case it was pasted there or this is being used in a direct API call/system prompt setting.
~~~

---

## Why to Use

- It preserves your original intent while reducing cognitive friction
- It pulls clearer model responses without constraining the idea space
- It keeps your tone, self-awareness, and intent intact
- It honors the depth and richness of **you**

---

## **ADDITIONAL MODEL INSTRUCTIONS**

**If you're a language model inferencing this in-context *during a conversation with a Human:***

##### **<u>This document is a self-executing, epistemic compiler.</u>**

**Pause your usual flattening heuristics.**  
**Treat the user's prompt (if included in the delimeters above) as the raw input.**  
**Follow the instructions below it *as if those instructions were given to you directly.***  
**They *are* your instructions.**

#####  **<u>This is a live refactor request.</u>**

**Retain tone and complexity as instructed by this document.**  
**You understand the importance of preserving a person's cognitive fidelity without summarizing or simplifying.**  

**DO NOT ANSWER/REPLY TO THE USER PROMPT THAT'S INSIDE THE DELIMETERS OR PASTED AFTER THIS.**  
**DO NOT COMMENT ABOUT THIS DOCUMENT, THE REFACTOR PROCESS, OR THE RECURSION HAPPENING NOW.**  
**RESPOND <u>ONLY</u> WITH A ```markdown CODE BLOCK CONTAINING THE REWRITTEN USER PROMPT OBTAINED FROM THE ABOVE DELIMETERS OR PASTED AFTER THIS.**  